# FedGIEï¼šAnalytical Layer-wise Decomposition with Moore-Penrose Pseudoinverse for Stable Gradient-free Federated Learning

**FedGIE** is a gradient-free federated learning framework. Each layer update is solved as a **least-squares** problem with a **Mooreâ€“Penrose pseudoinverse**, avoiding backpropagation and black-box gradient estimation. A top-down **feedback projection** plus a ReLU **diagonal Jacobian correction** stabilizes update directions under strong Non-IID data. The repository includes both **MLP** and **CNN** reference models and supports **MNIST**, **Fashion-MNIST**, and **CIFAR-10**.

> All `.py` sources are intentionally **comment-free** as requested.

> Manuscript status: This work has been submitted to Science China Information Sciences and is currently under peer review.

---

## âœ¨ Features

- Closed-form per-layer updates (weights & bias via least squares with pseudoinverse).
- Top-down feedback projection to supervise lower layers without gradients.
- Activation-aware correction (diagonal Jacobian for ReLU).
- CNN support using `unfold/fold` to linearize convolutions for closed-form solutions.
- Federated training loop with broadcast + parameter averaging.
- Configurable Non-IID partitions via Dirichlet sampling.

---

## ðŸ§± Repository Layout

```
fedgie-multi/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ train.py
â””â”€â”€ fedgie/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ utils.py
    â”œâ”€â”€ server.py
    â”œâ”€â”€ client.py
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ partition.py
    â””â”€â”€ models/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ mlp.py
        â””â”€â”€ cnn.py
```
- `train.py`: entrypoint (CLI, initialization, training, evaluation)
- `fedgie/server.py`: global model, broadcast, aggregation, evaluation
- `fedgie/client.py`: client-side closed-form local updates (Linear + Conv2d)
- `fedgie/models/`: MLP and CNN reference models
- `fedgie/data/partition.py`: datasets and Dirichlet Non-IID partitioning

---

## ðŸ”§ Installation

**Requirements**
- Python â‰¥ 3.9
- PyTorch and TorchVision (CPU or CUDA builds)

```bash
python -m venv .venv
. .venv/bin/activate                # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```
If you need GPU acceleration, install a CUDA-matching PyTorch wheel per the official PyTorch instructions, then install `torchvision`.

---

## ðŸš€ Quick Start

### MLP + MNIST
```bash
python train.py --dataset mnist --model mlp --clients 20 --rounds 100 --batch 32 --alpha 0.6
```

### CNN + MNIST
```bash
python train.py --dataset mnist --model cnn --clients 20 --rounds 100 --batch 32 --alpha 0.6
```

### CNN + CIFAR-10
```bash
python train.py --dataset cifar10 --model cnn --clients 20 --rounds 100 --batch 32 --alpha 0.6
```
TorchVision will auto-download datasets into `./data/`.

---

## âš™ï¸ Command-Line Arguments

| Argument     | Type   | Default | Description                                                                     |
|--------------|--------|---------|---------------------------------------------------------------------------------|
| `--dataset`  | str    | `mnist` | One of: `mnist`, `fashion_mnist`, `cifar10`.                                   |
| `--model`    | str    | `mlp`   | One of: `mlp`, `cnn`.                                                           |
| `--clients`  | int    | `20`    | Number of clients.                                                              |
| `--rounds`   | int    | `100`   | Number of federated rounds.                                                     |
| `--batch`    | int    | `32`    | Local batch size per client update.                                             |
| `--alpha`    | float  | `0.6`   | Dirichlet Non-IID strength (smaller = more skewed).                            |
| `--seed`     | int    | `42`    | Random seed.                                                                    |
| `--device`   | str    | `auto`  | `cpu`, `cuda`, or `auto` (use GPU if available).                                |

---

## ðŸ§  Method Overview

**Goal.** Avoid unstable black-box gradient estimation in federated settings by replacing backprop with structured, per-layer least-squares updates.

**Per-round, per-client outline:**
1. Run a single forward pass and cache each layerâ€™s input `h` and pre-activation `z`.
2. At the top layer, define a target matrix `F` (e.g., one-hot labels, spatially broadcast for CNN).
3. Solve a bias-augmented linear regression in closed form:
   - Build `Ä¤ = [1; Háµ€]`, compute `Å´ = F Â· pinv(Ä¤)`.
   - Extract `W = Å´[:,1:]`, `b = Å´[:,0]`.
4. Compute a top-down feedback signal for the previous layer by pseudo-inverting the updated mapping and apply ReLU diagonal Jacobian (element-wise mask on positive pre-activations).
5. Repeat for all layers down to the input.
6. Return local weights to the server; the server averages parameters to form the new global model.

**CNN specifics.** For `Conv2d`, use `torch.nn.functional.unfold` to produce local receptive-field matrices, solve the linear system in closed form, then use `fold` to project the feedback back to feature maps.

---

## ðŸ“Š Datasets & Partitioning

- Datasets: MNIST, Fashion-MNIST, CIFAR-10 (auto-downloaded to `./data/`).
- Non-IID Split: Dirichlet(Î±) over label distributions into `--clients` partitions.
  - Lower `alpha` â†’ stronger heterogeneity.

---

## ðŸ”Ž Reproducibility

- Use `--seed` to fix randomness.
- The script prints test accuracy each round:
  ```
  round=1 acc=0.8123
  round=2 acc=0.8410
  ...
  ```
- Tip: redirect logs for analysis:
  ```bash
  python train.py ... | tee run.log
  ```

---

## ðŸ§© Extending the Project

**Add a new model**
- Create a file under `fedgie/models/` (e.g., `resnet.py`) exposing:
  - `layers`: list of modules to be updated in order (e.g., `Linear`/`Conv2d`).
  - `activations`: list of activation names aligned with `layers` (e.g., `["relu","relu","none"]`).
  - `forward(x)` and `forward_cache(x)` returning `(h_list, z_list)`.

**Add a new dataset**
- Extend `get_dataset` in `fedgie/data/partition.py` to return `(train, test, num_classes, in_dim_or_none)`.

---

## âš ï¸ Known Limitations

- Memory/compute: `torch.linalg.pinv` may be heavy for large layers; reduce `--batch` or model width if needed.
- Pooling/strides: The CNN example focuses on a minimal consistent setup. When adding pooling or different strides/dilations, ensure `unfold/fold` parameters exactly match the convolution configuration.
- Aggregation: Default is uniform parameter averaging; you may replace it with data-size weighted averaging.

---

## ðŸ“¦ Requirements

`requirements.txt` contains:
```
torch
torchvision
```
For GPU builds, install CUDA-compatible wheels as per PyTorchâ€™s official guide.

---

## â“ FAQ

**Q: Why no backprop or optimizer?**  
A: Each layer update is a closed-form least-squares solution, so no gradient steps are needed.

**Q: How is the classification target formed?**  
A: We use one-hot labels (or their spatially broadcast version for CNN), then propagate top-down with pseudoinverse and activation-aware correction.

**Q: Does it support GPU?**  
A: Yes. Set `--device cuda` or leave `--device auto` to use GPU if available.

---

## ðŸ“œ License & Citation

- License: Add a `LICENSE` file of your choice (e.g., MIT) at the repository root.
- Citation: If this repository is useful in your research or product, please cite it. Example:
```bibtex
@misc{fedgie2025,
  title        = {Analytical Layer-wise Decomposition with Moore--Penrose Pseudoinverse for Stable Gradient-Free Federated Learning},
  author       = {Ruoyan XIONG, Yuepeng LI, Zhexiong LI, Lin GU, Deze ZENG, Quan CHEN & Minyi GUO},
  year         = {2025},
  note         = {Code available at: https://github.com/AINetworkLab/FedGIE}
}
```

---

## ðŸ“¬ Contact

> For enhancements (e.g., residual blocks, batch normalization, sparsity, asynchronous aggregation) or issues, please open a GitHub Issue.

> Please feel free to contact us at ryxiong@cug.edu.cn
